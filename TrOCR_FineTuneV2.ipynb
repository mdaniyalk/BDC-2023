{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrOCR Fine Tune\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q torch torchvision torchaudio\n",
    "# %pip install -q datasets jiwer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, itertools\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "# export CUDA_LAUNCH_BLOCKING=1\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor, default_data_collator\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "from preprocess_image import PreprocessImage\n",
    "from augment_image import AugmentImage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingest, Preprocess, & Split Dataset (into Training & Testing Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicleregistrationplate</th>\n",
       "      <th>NameofFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A7814</td>\n",
       "      <td>DataTrain1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1074QO</td>\n",
       "      <td>DataTrain2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1031QO</td>\n",
       "      <td>DataTrain3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B187EDA</td>\n",
       "      <td>DataTrain4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1089VD</td>\n",
       "      <td>DataTrain5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Vehicleregistrationplate      NameofFile\n",
       "0                    A7814  DataTrain1.png\n",
       "1                  B1074QO  DataTrain2.png\n",
       "2                  B1031QO  DataTrain3.png\n",
       "3                  B187EDA  DataTrain4.png\n",
       "4                  B1089VD  DataTrain5.png"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train/DataTrain.csv', delimiter=';')\n",
    "df_train = df_train.drop(['Unnamed: 0'], axis=1)\n",
    "df_train = df_train.drop([126, 457, 600]) # delete the wrong labeled data\n",
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show First Samples in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicleregistrationplate</th>\n",
       "      <th>NameofFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A7814</td>\n",
       "      <td>DataTrain1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1074QO</td>\n",
       "      <td>DataTrain2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1031QO</td>\n",
       "      <td>DataTrain3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B187EDA</td>\n",
       "      <td>DataTrain4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1089VD</td>\n",
       "      <td>DataTrain5.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B1972RBP</td>\n",
       "      <td>DataTrain6.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AB2400WU</td>\n",
       "      <td>DataTrain7.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AB6268YQ</td>\n",
       "      <td>DataTrain8.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A8014VA</td>\n",
       "      <td>DataTrain9.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B1554EJA</td>\n",
       "      <td>DataTrain10.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B1160BH</td>\n",
       "      <td>DataTrain11.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B1032PJE</td>\n",
       "      <td>DataTrain12.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vehicleregistrationplate       NameofFile\n",
       "0                     A7814   DataTrain1.png\n",
       "1                   B1074QO   DataTrain2.png\n",
       "2                   B1031QO   DataTrain3.png\n",
       "3                   B187EDA   DataTrain4.png\n",
       "4                   B1089VD   DataTrain5.png\n",
       "5                  B1972RBP   DataTrain6.png\n",
       "6                  AB2400WU   DataTrain7.png\n",
       "7                  AB6268YQ   DataTrain8.png\n",
       "8                   A8014VA   DataTrain9.png\n",
       "9                  B1554EJA  DataTrain10.png\n",
       "10                  B1160BH  DataTrain11.png\n",
       "11                 B1032PJE  DataTrain12.png"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the image and label dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Label & Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = df_train['Vehicleregistrationplate'].to_numpy()\n",
    "path_train = df_train['NameofFile'].to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train val split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess & Augment Train Images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDC_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_array, label_array, processor, max_target_length=12):\n",
    "        self.img_array = img_array\n",
    "        self.label_array = label_array\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get iimage + label\n",
    "        image = self.img_array[idx]\n",
    "        label = self.label_array[idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.to('mps:0')\n",
    "        # add labels (input_ids) by encoding the label\n",
    "        labels = self.processor.tokenizer(label, padding=\"max_length\", max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id \n",
    "                  else -100 for label in labels]\n",
    "        \n",
    "        encoding = {\"pixel_values\" : pixel_values.squeeze(), \"labels\" : torch.tensor(labels).to('mps:0')}\n",
    "        return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Values/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"microsoft/trocr-base-printed\"\n",
    "MODEL_NAME =  MODEL_CKPT.split(\"/\")[-1] + \"_bdc_license_plates_ocrV2\"\n",
    "NUM_OF_EPOCHS = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate Processor, Create Training, & Testing Dataset Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login --token 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"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit/Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import character_accuracy_np as char_acc \n",
    "\n",
    "def train_model(image_paths, label, num_epoch):\n",
    "    \n",
    "    train_path_array, test_path_array, train_label_array, test_label_array = train_test_split(image_paths, label, test_size=0.2, random_state=42)\n",
    "    test_image = PreprocessImage(img_paths=test_path_array, output_pixel=384, padding_ratio=0.9, path_prefix='train', include_original=False)\n",
    "    test_img_array = test_image.img_array.copy()\n",
    "    train_image = PreprocessImage(img_paths=train_path_array, output_pixel=384, padding_ratio=0.9, path_prefix='train', include_original=True)\n",
    "    train_img_array = train_image.img_array.copy()\n",
    "\n",
    "    processor = TrOCRProcessor.from_pretrained(MODEL_CKPT)\n",
    "    \n",
    "\n",
    "    test_ds = BDC_Dataset(test_img_array, test_label_array, processor=processor)\n",
    "    \n",
    "    print(f\"The testing dataset has {len(test_ds)} samples in it.\")\n",
    "\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(MODEL_CKPT)\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "    model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "    model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "    model.config.max_length = 12\n",
    "    model.config.early_stopping = True\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "    model.config.length_penalty = 2.0\n",
    "    model.config.num_beams = 4\n",
    "\n",
    "    cer_metric = load_metric(\"cer\")\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        label_ids = pred.label_ids\n",
    "        pred_ids = pred.predictions\n",
    "\n",
    "        pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "        label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "        \n",
    "        cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "        \n",
    "        tmp_hat_test = [np.asarray(list(x)) for x in pred_str]\n",
    "        pred_str = np.asarray(tmp_hat_test)\n",
    "        tmp_label_test = [np.asarray(list(x)) for x in label_str]\n",
    "        label_str = np.asarray(tmp_label_test)\n",
    "        acc = char_acc(label_str, pred_str)\n",
    "\n",
    "        return {\"char_acc\": acc, \"cer\" : cer}\n",
    "    \n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir = MODEL_NAME,\n",
    "        num_train_epochs=1,\n",
    "        predict_with_generate=True,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_first_step=True,\n",
    "        # hub_private_repo=True,\n",
    "        # push_to_hub=True,\n",
    "        use_mps_device=False,\n",
    "        fp16=False,\n",
    "        no_cuda=True\n",
    "    )\n",
    "\n",
    "    for i in range(num_epoch):\n",
    "        if i == 0:\n",
    "            train_image_array = train_img_array\n",
    "        augmenter = AugmentImage(image_array=train_img_array, label_array=train_label_array, num_augmentations=3-i)\n",
    "        train_image_array, train_lbl_array = augmenter.transform()\n",
    "        train_ds = BDC_Dataset(train_image_array, train_lbl_array, processor=processor)\n",
    "        print(f\"The training dataset has {len(train_ds)} samples in it.\")\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model.to('mps:0'),\n",
    "            tokenizer=processor.feature_extractor,\n",
    "            args=args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=test_ds,\n",
    "            data_collator=default_data_collator,\n",
    "            \n",
    "        )\n",
    "        train_results = trainer.train()\n",
    "    trainer.save_model()\n",
    "    trainer.log_metrics(\"train\", train_results.metrics)\n",
    "    trainer.save_metrics(\"train\", train_results.metrics)\n",
    "    trainer.save_state()\n",
    "    metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Preprocess: Add Padding:   0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Preprocess: Add Padding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:00<00:00, 654.69it/s]\n",
      "Initial Preprocess: Add Padding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 637/637 [00:00<00:00, 786.99it/s]\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing dataset has 160 samples in it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/vt/q06w72ls4kz7lg_wsqqw2f0h0000gp/T/ipykernel_66749/2959376207.py:31: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n",
      "/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
      "  warn_deprecated(msg, stacklevel=3)\n",
      "Augement Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 637/637 [00:18<00:00, 33.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 1911 samples in it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/transformers/models/trocr/processing_trocr.py:134: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 1/239 [00:12<48:26, 12.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0071, 'learning_rate': 4.97907949790795e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 165/239 [53:21<31:27, 25.50s/it]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(path_train, label_array, \u001b[39m2\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[9], line 82\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(image_paths, label, num_epoch)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe training dataset has \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_ds)\u001b[39m}\u001b[39;00m\u001b[39m samples in it.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m     trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     73\u001b[0m         model\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mmps:0\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     74\u001b[0m         tokenizer\u001b[39m=\u001b[39mprocessor\u001b[39m.\u001b[39mfeature_extractor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \n\u001b[1;32m     81\u001b[0m     )\n\u001b[0;32m---> 82\u001b[0m     train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     83\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n\u001b[1;32m     84\u001b[0m trainer\u001b[39m.\u001b[39mlog_metrics(\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, train_results\u001b[39m.\u001b[39mmetrics)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1937\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1938\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1940\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1941\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1942\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1943\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1944\u001b[0m ):\n\u001b[1;32m   1945\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/transformers/trainer.py:2770\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2768\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2769\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2770\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2772\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/accelerate/accelerator.py:1821\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1820\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1821\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(path_train, label_array, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_eval(trainer, processor, dataset):\n",
    "    output = trainer.predict(dataset)\n",
    "    outputs = []\n",
    "    for out in output.predictions:\n",
    "        generated_text = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
    "        outputs.append(generated_text)\n",
    "    return np.asarray(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import character_accuracy_np as char_acc \n",
    "\n",
    "# yhat_train = manual_eval(trainer=trainer, processor=processor, dataset = train_ds)\n",
    "# print(f\"Train dataset char_acc = {char_acc(train_label_array, yhat_train)}\")\n",
    "\n",
    "yhat_test = manual_eval(trainer=trainer, processor=processor, dataset = test_ds)\n",
    "print(f\"test dataset char_acc = {char_acc(test_label_array, yhat_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def manual_eval(model, processor, images):\n",
    "    output = []\n",
    "    px_vals = []\n",
    "    for image in tqdm(images):\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to('cpu')\n",
    "        px_vals.append(pixel_values)\n",
    "    for pixel_values in tqdm(px_vals):\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        output.append(generated_text)\n",
    "    return np.asarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdc_model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import character_accuracy_np\n",
    "\n",
    "# yhat_train = manual_eval(model=bdc_model, processor=processor, images = train_image_array)\n",
    "# tmp_hat = [np.asarray(list(x)) for x in yhat_train]\n",
    "# yhat_train = np.asarray(tmp_hat)\n",
    "# print(f\"Train dataset char_acc = {character_accuracy_np(train_label_array, yhat_train)}\")\n",
    "\n",
    "yhat_test = manual_eval(model=bdc_model, processor=processor, images = test_img_array)\n",
    "tmp_hat_test = [np.asarray(list(x)) for x in yhat_test]\n",
    "yhat_test = np.asarray(tmp_hat_test)\n",
    "tmp_label_test = [np.asarray(list(x)) for x in test_label_array]\n",
    "ylabel_test = np.asarray(tmp_label_test)\n",
    "print(f\"test dataset char_acc = {character_accuracy_np(ylabel_test, yhat_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes & Other Takeaways From This Project\n",
    "****\n",
    "- The results were pretty good. I was pondering whether to train for 2 or 3 epochs. Ultimately, I trained this model for 2 epochs. If this were a work project (where multiprocessing and other options are available), I would have trained for 3, if not 4, epochs.\n",
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "##### For Transformer Checkpoint\n",
    "- @misc{li2021trocr,\n",
    "      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \n",
    "      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\n",
    "      year={2021},\n",
    "      eprint={2109.10282},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL}\n",
    "}\n",
    "\n",
    "##### For CER Metric\n",
    "- @inproceedings{morris2004,\n",
    "author = {Morris, Andrew and Maier, Viktoria and Green, Phil},\n",
    "year = {2004},\n",
    "month = {01},\n",
    "pages = {},\n",
    "title = {From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition.}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41bc52750e0704433c7c40a5c68d8f60e760babe95f2dffc82e8c3790208ff57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
