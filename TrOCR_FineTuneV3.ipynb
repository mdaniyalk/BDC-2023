{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrOCR Fine Tune\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q torch torchvision torchaudio\n",
    "# %pip install -q datasets jiwer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, itertools\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "# export CUDA_LAUNCH_BLOCKING=1\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor, default_data_collator\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "from preprocess_image import PreprocessImage\n",
    "from augment_image import AugmentImage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingest, Preprocess, & Split Dataset (into Training & Testing Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicleregistrationplate</th>\n",
       "      <th>NameofFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A7814</td>\n",
       "      <td>DataTrain1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1074QO</td>\n",
       "      <td>DataTrain2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1031QO</td>\n",
       "      <td>DataTrain3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B187EDA</td>\n",
       "      <td>DataTrain4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1089VD</td>\n",
       "      <td>DataTrain5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Vehicleregistrationplate      NameofFile\n",
       "0                    A7814  DataTrain1.png\n",
       "1                  B1074QO  DataTrain2.png\n",
       "2                  B1031QO  DataTrain3.png\n",
       "3                  B187EDA  DataTrain4.png\n",
       "4                  B1089VD  DataTrain5.png"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train/DataTrain.csv', delimiter=';')\n",
    "df_train = df_train.drop(['Unnamed: 0'], axis=1)\n",
    "df_train = df_train.drop([126, 457, 600]) # delete the wrong labeled data\n",
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show First Samples in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicleregistrationplate</th>\n",
       "      <th>NameofFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A7814</td>\n",
       "      <td>DataTrain1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1074QO</td>\n",
       "      <td>DataTrain2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1031QO</td>\n",
       "      <td>DataTrain3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B187EDA</td>\n",
       "      <td>DataTrain4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1089VD</td>\n",
       "      <td>DataTrain5.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B1972RBP</td>\n",
       "      <td>DataTrain6.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AB2400WU</td>\n",
       "      <td>DataTrain7.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AB6268YQ</td>\n",
       "      <td>DataTrain8.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A8014VA</td>\n",
       "      <td>DataTrain9.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B1554EJA</td>\n",
       "      <td>DataTrain10.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B1160BH</td>\n",
       "      <td>DataTrain11.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B1032PJE</td>\n",
       "      <td>DataTrain12.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vehicleregistrationplate       NameofFile\n",
       "0                     A7814   DataTrain1.png\n",
       "1                   B1074QO   DataTrain2.png\n",
       "2                   B1031QO   DataTrain3.png\n",
       "3                   B187EDA   DataTrain4.png\n",
       "4                   B1089VD   DataTrain5.png\n",
       "5                  B1972RBP   DataTrain6.png\n",
       "6                  AB2400WU   DataTrain7.png\n",
       "7                  AB6268YQ   DataTrain8.png\n",
       "8                   A8014VA   DataTrain9.png\n",
       "9                  B1554EJA  DataTrain10.png\n",
       "10                  B1160BH  DataTrain11.png\n",
       "11                 B1032PJE  DataTrain12.png"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the image and label dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Label & Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = df_train['Vehicleregistrationplate'].to_numpy()\n",
    "path_train = df_train['NameofFile'].to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_array, test_path_array, train_label_array, test_label_array = train_test_split(path_train, label_array, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Preprocess: Add Padding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:00<00:00, 538.31it/s]\n"
     ]
    }
   ],
   "source": [
    "test_image = PreprocessImage(img_paths=test_path_array, output_pixel=384, padding_ratio=0.9, path_prefix='train', include_original=False)\n",
    "test_img_array = test_image.img_array.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess & Augment Train Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label_array = np.concatenate((train_label_array, train_label_array), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 12\n",
    "DEVICE = 'mps:0'\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDC_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_array, label_array, processor, max_target_length=MAX_LENGTH):\n",
    "        self.img_array = img_array\n",
    "        self.label_array = label_array\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get iimage + label\n",
    "        image = self.img_array[idx]\n",
    "        label = self.label_array[idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "        # add labels (input_ids) by encoding the label\n",
    "        labels = self.processor.tokenizer(label, padding=\"max_length\", max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id \n",
    "                  else -100 for label in labels]\n",
    "        \n",
    "        encoding = {\"pixel_values\" : pixel_values.squeeze(), \"labels\" : torch.tensor(labels).to(DEVICE)}\n",
    "        return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Values/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"microsoft/trocr-base-printed\"\n",
    "MODEL_NAME =  MODEL_CKPT.split(\"/\")[-1] + \"_bdc_license_plates_ocrV3\"\n",
    "NUM_OF_EPOCHS = 2\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate Processor, Create Training, & Testing Dataset Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "\n",
    "test_ds = BDC_Dataset(test_img_array, test_label_array, processor=processor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print Length of Training & Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing dataset has 160 samples in it.\n"
     ]
    }
   ],
   "source": [
    "# print(f\"The training dataset has {len(train_ds)} samples in it.\")\n",
    "print(f\"The testing dataset has {len(test_ds)} samples in it.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of Input Data Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = train_ds[0]\n",
    "\n",
    "# for k,v in encoding.items():\n",
    "#     print(k, \" : \", v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(train_ds.root_dir + train_dataset['file_name'][0]).convert(\"RGB\")\n",
    "\n",
    "# image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show Label for Above Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = encoding['labels']\n",
    "# labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "# label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "# print(label_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "eval_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Configuration Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = MAX_LENGTH\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/q06w72ls4kz7lg_wsqqw2f0h0000gp/T/ipykernel_20509/3840881219.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n"
     ]
    }
   ],
   "source": [
    "from metrics import character_accuracy_np as char_acc \n",
    "\n",
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "def compute_metrics(pred_ids, label_ids):\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # tmp_hat_test = [np.asarray(list(x)) for x in pred_str]\n",
    "    # pred_str = np.asarray(tmp_hat_test)\n",
    "    # tmp_label_test = [np.asarray(list(x)) for x in label_str]\n",
    "    # label_str = np.asarray(tmp_label_test)\n",
    "    # acc = char_acc(label_str, pred_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def manual_eval(model, processor, images):\n",
    "    output = []\n",
    "    px_vals = []\n",
    "    for image in tqdm(images):\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "        px_vals.append(pixel_values)\n",
    "    for pixel_values in tqdm(px_vals):\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        generated_text = generated_text.replace(\" \", \"\") # Remove spacing\n",
    "        output.append(generated_text)\n",
    "    return np.asarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Preprocess: Add Padding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 637/637 [00:00<00:00, 777.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape= (637, 384, 384, 3), label= (637,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess: Convert to BW & High Contrast Image: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 637/637 [00:00<00:00, 4252.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bw_image image shape= (608, 384, 384, 3), label= (608,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess: Convert to Grayscale Image: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 637/637 [00:00<00:00, 7503.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gray_image image shape= (636, 384, 384, 3), label= (636,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocess: Segment Number Image: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 637/637 [00:00<00:00, 733.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment_number image shape= (466, 384, 384, 3), label= (466,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "masked_img = {'bw_image': np.array([10, 41, 23, 15, 16, 7, 28, 81, 73, 83, 59, 89, 100, 112, 148, 152, 214, 250, 292, 309, 385, 410, 434, 419, 446, 490, 464, 505, 566]),\n",
    "              'gray_image': np.array([521]),\n",
    "              'segment_number': np.array([2, 6, 7, 8, 9, 15, 16, 28, 29, 39, 37, 33, 42, 23, 21, 20, 80, 81, 82, 83, 74, 71, 54, 69, 59, 77, 79, 89, 99, 96, 85, 104, 105, 106, 114, 115, 116, 117, 119, 124, 125, 128, 130, 138, 139, 148, 149, 152, 153, 154, 156, 158, 159, 165, 167, 177, 180, 182, 197, 201, 202, 203, 204, 214, 216, 220, 223, 227, 235, 239, 243, 247, 248, 250, 251, 252, 258, 262, 265, 271, 273, 279, 288, 292, 294, 297, 299, 310, 307, 313, 315, 319, 320, 321, 323, 324, 330, 331, 332, 333, 335, 343, 345, 348, 350, 357, 360, 363, 364, 369, 370, 377, 381, 383, 384, 385, 389, 396, 398, 399, 402, 416, 419, 422, 426, 428, 429, 434, 440, 442, 450, 458, 459, 460, 461, 462, 464, 466, 469, 473, 478, 479, 486, 490, 505, 506, 521, 528, 529, 531, 539, 550, 551, 553, 557, 563, 566, 568, 579, 589, 585, 592, 594, 597, 595, 599, 604, 606, 608, 629, 628]),\n",
    "              }\n",
    "train_image = PreprocessImage(img_paths=train_path_array, output_pixel=384, padding_ratio=0.9, path_prefix='train', include_original=False)\n",
    "# train_image_array = train_image.gray_image(masked_img=np.array([]))\n",
    "original = train_image.img_array.copy()\n",
    "original_label = train_label_array.copy()\n",
    "print(f'Original image shape= {original.shape}, label= {original_label.shape}')\n",
    "bw_image = train_image.bw_image(masked_img['bw_image'])\n",
    "bw_image_label = train_label_array.copy()\n",
    "bw_image_label = np.delete(bw_image_label, masked_img['bw_image'])\n",
    "print(f'bw_image image shape= {bw_image.shape}, label= {bw_image_label.shape}')\n",
    "gray_image = train_image.gray_image(masked_img['gray_image'])\n",
    "gray_image_label = train_label_array.copy()\n",
    "gray_image_label = np.delete(gray_image_label, masked_img['gray_image'])\n",
    "print(f'gray_image image shape= {gray_image.shape}, label= {gray_image_label.shape}')\n",
    "segment_number = train_image.segment_number_image(masked_img['segment_number'])\n",
    "segment_number_label = train_label_array.copy()\n",
    "segment_number_label = np.delete(segment_number_label, masked_img['segment_number'])\n",
    "print(f'segment_number image shape= {segment_number.shape}, label= {segment_number_label.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_list = [bw_image, gray_image, segment_number]\n",
    "train_label_list = [bw_image_label, gray_image_label, segment_number_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augement Images: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 637/637 [00:07<00:00, 79.99it/s]\n",
      "Epoch: 1/2:  10%|â–ˆ         | 32/319 [00:56<08:25,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 31: 2.528225202714243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2:  20%|â–ˆâ–‰        | 63/319 [01:47<06:55,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 62: 2.211847561021005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2:  29%|â–ˆâ–ˆâ–‰       | 94/319 [02:44<06:20,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 93: 2.0924722597163212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 125/319 [03:39<05:59,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 124: 2.0330332575305814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 156/319 [04:30<04:23,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 155: 1.990711002196035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 187/319 [05:21<03:44,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 186: 1.951682622073799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 218/319 [06:12<02:47,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 217: 1.9319304008088354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 249/319 [07:05<02:02,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 248: 1.9268098050548184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 280/319 [08:01<01:07,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 279: 1.9042944433868572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 311/319 [09:00<00:14,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 310: 1.8943292779307213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 319/319 [09:14<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 1.8886803235379879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:01<00:00, 106.89it/s]\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (12) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/transformers/generation/utils.py:723: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
      "/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/transformers/generation/beam_search.py:357: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  sent_lengths_max = sent_lengths.max().item() + 1\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [02:59<00:00,  1.12s/it]\n",
      "/var/folders/vt/q06w72ls4kz7lg_wsqqw2f0h0000gp/T/ipykernel_20509/852598997.py:69: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ylabel_test = np.asarray(tmp_label_test)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (9,) into shape (6,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m tmp_label_test \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39masarray(\u001b[39mlist\u001b[39m(x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m test_label_array]\n\u001b[1;32m     69\u001b[0m ylabel_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(tmp_label_test)\n\u001b[0;32m---> 70\u001b[0m valid_acc \u001b[39m=\u001b[39m char_acc(ylabel_test, yhat_test)\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(eval_dataloader):\n\u001b[1;32m     73\u001b[0m     \u001b[39m# run batch generation\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(batch[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(DEVICE))\n",
      "File \u001b[0;32m~/Documents/github/work/BDC-2023/metrics.py:31\u001b[0m, in \u001b[0;36mcharacter_accuracy_np\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_pred[\u001b[39mid\u001b[39m]) \u001b[39m<\u001b[39m \u001b[39m9\u001b[39m:\n\u001b[1;32m     30\u001b[0m     pad_length \u001b[39m=\u001b[39m \u001b[39m9\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(y_pred[\u001b[39mid\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m     y_pred[\u001b[39mid\u001b[39;49m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(y_pred[\u001b[39mid\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m pad_length)\n\u001b[1;32m     32\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(y_pred[\u001b[39mid\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m9\u001b[39m:\n\u001b[1;32m     33\u001b[0m     y_pred[\u001b[39mid\u001b[39m] \u001b[39m=\u001b[39m y_pred[\u001b[39mid\u001b[39m][:\u001b[39m9\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (9,) into shape (6,)"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "max_epoch = 2\n",
    "\n",
    "for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "    \n",
    "    # Generate augmented image data for each epoch\n",
    "    if epoch>=0:\n",
    "        augmenter = AugmentImage(image_array=original, label_array=original_label, num_augmentations=1)\n",
    "        augment_original, augment_original_label = augmenter.transform()\n",
    "        # new_train_image_list = train_image_list.copy()\n",
    "        # new_train_label_list = train_label_list.copy()\n",
    "        # for i in range(len(train_image_list)):\n",
    "        #     augmenter = AugmentImage(image_array=train_image_list[i], label_array=train_label_list[i], num_augmentations=2+epoch)\n",
    "        #     tmp_array, tmp_array_label = augmenter.transform()\n",
    "        #     random_indices = np.random.choice(train_image_list[i].shape[0], size=train_image_list[i].shape[0]//(2*(epoch+2)), replace=False)\n",
    "        #     new_train_image_list[i] = tmp_array[random_indices]\n",
    "        #     new_train_label_list[i] = tmp_array_label[random_indices]\n",
    "        # new_train_image_list.append(augment_original)\n",
    "        # new_train_label_list.append(augment_original_label)\n",
    "        # train_image_array = np.concatenate(new_train_image_list, axis=0)\n",
    "        # train_lbl_array = np.concatenate(new_train_label_list, axis=0)\n",
    "        # augmenter = AugmentImage(image_array=train_img_array, label_array=train_label_array, num_augmentations=2)\n",
    "        # train_image_array, train_lbl_array = augmenter.transform()\n",
    "    # elif epoch < 3:\n",
    "    #     train_img_array = np.concatenate((train_image_list[epoch], original), axis=0)\n",
    "    #     train_label_array = np.concatenate((train_label_list[epoch], original_label), axis=0)\n",
    "    #     augmenter = AugmentImage(image_array=train_img_array, label_array=train_label_array, num_augmentations=2)\n",
    "    #     train_image_array, train_lbl_array = augmenter.transform()\n",
    "    # else:\n",
    "    #     augmenter = AugmentImage(image_array=original, label_array=original_label, num_augmentations=2)\n",
    "    #     train_image_array, train_lbl_array = augmenter.transform()\n",
    "    train_ds = BDC_Dataset(augment_original, augment_original_label, processor=processor)\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader, desc=f'Epoch: {epoch+1}/{max_epoch}')):\n",
    "        # get the inputs\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(DEVICE)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if idx % (len(train_dataloader)//10) == 0 and idx != 0:\n",
    "            print(f\"Loss after batch {idx}:\", train_loss/idx)\n",
    "\n",
    "    print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    valid_cer = 0.0\n",
    "    valid_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        yhat_test = manual_eval(model=model, processor=processor, images = test_img_array)\n",
    "        tmp_hat_test = [np.asarray(list(x)) for x in yhat_test]\n",
    "        yhat_test = np.asarray(tmp_hat_test)\n",
    "        tmp_label_test = [np.asarray(list(x)) for x in test_label_array]\n",
    "        ylabel_test = np.asarray(tmp_label_test)\n",
    "        valid_acc = char_acc(ylabel_test, yhat_test)\n",
    "\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            # run batch generation\n",
    "            outputs = model.generate(batch[\"pixel_values\"].to(DEVICE))\n",
    "            # compute metrics\n",
    "            cer = compute_metrics(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            valid_cer += cer \n",
    "        \n",
    "\n",
    "    print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
    "    print(\"Validation Char_acc:\", valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '6', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S'],\n",
       "       ['B', '1', '8', '5', '8', 'S']], dtype='<U1')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 387, 1225, 846, 1889, 2, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "processor.tokenizer(augment_original_label[1], padding=\"max_length\", max_length=12).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B1472PQH'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_original_label[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def manual_eval(model, processor, images):\n",
    "    output = []\n",
    "    px_vals = []\n",
    "    for image in tqdm(images):\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        px_vals.append(pixel_values)\n",
    "    for pixel_values in tqdm(px_vals):\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        output.append(generated_text)\n",
    "    return np.asarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bdc_model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [00:01<00:00, 151.92it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160/160 [01:09<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset char_acc = 0.920138888888889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<ipython-input-24-b4c30051ea9a>:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  yhat_test = np.asarray(tmp_hat_test)\n",
      "<ipython-input-24-b4c30051ea9a>:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ylabel_test = np.asarray(tmp_label_test)\n"
     ]
    }
   ],
   "source": [
    "from metrics import character_accuracy_np\n",
    "\n",
    "# yhat_train = manual_eval(model=bdc_model, processor=processor, images = train_image_array)\n",
    "# tmp_hat = [np.asarray(list(x)) for x in yhat_train]\n",
    "# yhat_train = np.asarray(tmp_hat)\n",
    "# print(f\"Train dataset char_acc = {character_accuracy_np(train_label_array, yhat_train)}\")\n",
    "\n",
    "yhat_test = manual_eval(model=model, processor=processor, images = test_img_array)\n",
    "tmp_hat_test = [np.asarray(list(x)) for x in yhat_test]\n",
    "yhat_test = np.asarray(tmp_hat_test)\n",
    "tmp_label_test = [np.asarray(list(x)) for x in test_label_array]\n",
    "ylabel_test = np.asarray(tmp_label_test)\n",
    "print(f\"test dataset char_acc = {character_accuracy_np(ylabel_test, yhat_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Preprocess: Add Padding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 780.82it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 157.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:31<00:00,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset compared to GroundTruth char_acc = 0.8811111111111112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "<ipython-input-24-defe0c4decee>:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  yhat_test = np.asarray(tmp_hat_test)\n",
      "<ipython-input-24-defe0c4decee>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ylabel_test = np.asarray(tmp_label_test)\n"
     ]
    }
   ],
   "source": [
    "df_gt = pd.read_csv('ground_truth_manual.csv')\n",
    "gt = df_gt['Ground Truth'].to_numpy()\n",
    "gt_test_path_array = df_gt['Name of File'].to_numpy()\n",
    "\n",
    "gt_test_image = PreprocessImage(img_paths=gt_test_path_array, output_pixel=384, padding_ratio=0.9, path_prefix='test', include_original=False)\n",
    "gt_test_img_array = gt_test_image.img_array.copy()\n",
    "yhat_test = manual_eval(model=model, processor=processor, images = gt_test_img_array)\n",
    "tmp_hat_test = [np.asarray(list(x)) for x in yhat_test]\n",
    "yhat_test = np.asarray(tmp_hat_test)\n",
    "tmp_label_test = [np.asarray(list(x)) for x in gt]\n",
    "ylabel_test = np.asarray(tmp_label_test)\n",
    "print(f\"test dataset compared to GroundTruth char_acc = {char_acc(ylabel_test, yhat_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes & Other Takeaways From This Project\n",
    "****\n",
    "- The results were pretty good. I was pondering whether to train for 2 or 3 epochs. Ultimately, I trained this model for 2 epochs. If this were a work project (where multiprocessing and other options are available), I would have trained for 3, if not 4, epochs.\n",
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "##### For Transformer Checkpoint\n",
    "- @misc{li2021trocr,\n",
    "      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \n",
    "      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\n",
    "      year={2021},\n",
    "      eprint={2109.10282},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL}\n",
    "}\n",
    "\n",
    "##### For CER Metric\n",
    "- @inproceedings{morris2004,\n",
    "author = {Morris, Andrew and Maier, Viktoria and Green, Phil},\n",
    "year = {2004},\n",
    "month = {01},\n",
    "pages = {},\n",
    "title = {From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition.}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "41bc52750e0704433c7c40a5c68d8f60e760babe95f2dffc82e8c3790208ff57"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
