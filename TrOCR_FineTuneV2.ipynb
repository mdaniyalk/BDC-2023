{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrOCR Fine Tune\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q torch torchvision torchaudio\n",
    "# %pip install -q datasets jiwer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, itertools\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "# export CUDA_LAUNCH_BLOCKING=1\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor, default_data_collator\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "from preprocess_image import PreprocessImage\n",
    "from augment_image import AugmentImage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ingest, Preprocess, & Split Dataset (into Training & Testing Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicleregistrationplate</th>\n",
       "      <th>NameofFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A7814</td>\n",
       "      <td>DataTrain1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1074QO</td>\n",
       "      <td>DataTrain2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1031QO</td>\n",
       "      <td>DataTrain3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B187EDA</td>\n",
       "      <td>DataTrain4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1089VD</td>\n",
       "      <td>DataTrain5.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Vehicleregistrationplate      NameofFile\n",
       "0                    A7814  DataTrain1.png\n",
       "1                  B1074QO  DataTrain2.png\n",
       "2                  B1031QO  DataTrain3.png\n",
       "3                  B187EDA  DataTrain4.png\n",
       "4                  B1089VD  DataTrain5.png"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('train/DataTrain.csv', delimiter=';')\n",
    "df_train = df_train.drop(['Unnamed: 0'], axis=1)\n",
    "df_train = df_train.drop([126, 457, 600]) # delete the wrong labeled data\n",
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show First Samples in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vehicleregistrationplate</th>\n",
       "      <th>NameofFile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A7814</td>\n",
       "      <td>DataTrain1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1074QO</td>\n",
       "      <td>DataTrain2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1031QO</td>\n",
       "      <td>DataTrain3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B187EDA</td>\n",
       "      <td>DataTrain4.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1089VD</td>\n",
       "      <td>DataTrain5.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B1972RBP</td>\n",
       "      <td>DataTrain6.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AB2400WU</td>\n",
       "      <td>DataTrain7.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AB6268YQ</td>\n",
       "      <td>DataTrain8.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A8014VA</td>\n",
       "      <td>DataTrain9.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B1554EJA</td>\n",
       "      <td>DataTrain10.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B1160BH</td>\n",
       "      <td>DataTrain11.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B1032PJE</td>\n",
       "      <td>DataTrain12.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Vehicleregistrationplate       NameofFile\n",
       "0                     A7814   DataTrain1.png\n",
       "1                   B1074QO   DataTrain2.png\n",
       "2                   B1031QO   DataTrain3.png\n",
       "3                   B187EDA   DataTrain4.png\n",
       "4                   B1089VD   DataTrain5.png\n",
       "5                  B1972RBP   DataTrain6.png\n",
       "6                  AB2400WU   DataTrain7.png\n",
       "7                  AB6268YQ   DataTrain8.png\n",
       "8                   A8014VA   DataTrain9.png\n",
       "9                  B1554EJA  DataTrain10.png\n",
       "10                  B1160BH  DataTrain11.png\n",
       "11                 B1032PJE  DataTrain12.png"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the image and label dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Prepare Label & Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_array = df_train['Vehicleregistrationplate'].to_numpy()\n",
    "path_train = df_train['NameofFile'].to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_array, test_path_array, train_label_array, test_label_array = train_test_split(path_train, label_array, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Preprocess: Add Padding:   0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Preprocess: Add Padding: 100%|██████████| 160/160 [00:00<00:00, 685.78it/s]\n"
     ]
    }
   ],
   "source": [
    "test_image = PreprocessImage(img_paths=test_path_array, output_pixel=384, padding_ratio=0.9, path_prefix='train', include_original=False)\n",
    "test_img_array = test_image.img_array.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocess & Augment Train Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Preprocess: Add Padding: 100%|██████████| 637/637 [00:00<00:00, 709.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637, 384, 384, 3)\n"
     ]
    }
   ],
   "source": [
    "train_image = PreprocessImage(img_paths=train_path_array, output_pixel=384, padding_ratio=0.9, path_prefix='train', include_original=True)\n",
    "# train_image_array = train_image.gray_image(masked_img=np.array([]))\n",
    "train_img_array = train_image.img_array.copy()\n",
    "print(train_img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label_array = np.concatenate((train_label_array, train_label_array), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BDC_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_array, label_array, processor, max_target_length=12):\n",
    "        self.img_array = img_array\n",
    "        self.label_array = label_array\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get iimage + label\n",
    "        image = self.img_array[idx]\n",
    "        label = self.label_array[idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.to('mps:0')\n",
    "        # add labels (input_ids) by encoding the label\n",
    "        labels = self.processor.tokenizer(label, padding=\"max_length\", max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id \n",
    "                  else -100 for label in labels]\n",
    "        \n",
    "        encoding = {\"pixel_values\" : pixel_values.squeeze(), \"labels\" : torch.tensor(labels).to('mps:0')}\n",
    "        return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic Values/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"microsoft/trocr-base-printed\"\n",
    "MODEL_NAME =  MODEL_CKPT.split(\"/\")[-1] + \"_bdc_license_plates_ocrV2\"\n",
    "NUM_OF_EPOCHS = 2\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instantiate Processor, Create Training, & Testing Dataset Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "\n",
    "test_ds = BDC_Dataset(test_img_array, test_label_array, processor=processor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print Length of Training & Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing dataset has 160 samples in it.\n"
     ]
    }
   ],
   "source": [
    "# print(f\"The training dataset has {len(train_ds)} samples in it.\")\n",
    "print(f\"The testing dataset has {len(test_ds)} samples in it.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of Input Data Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = train_ds[0]\n",
    "\n",
    "# for k,v in encoding.items():\n",
    "#     print(k, \" : \", v.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(train_ds.root_dir + train_dataset['file_name'][0]).convert(\"RGB\")\n",
    "\n",
    "# image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show Label for Above Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = encoding['labels']\n",
    "# labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "# label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "# print(label_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "eval_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-printed and are newly initialized: ['encoder.pooler.dense.weight', 'encoder.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRLearnedPositionalEmbedding(514, 1024)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_CKPT)\n",
    "device = 'mps:0'\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Configuration Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 12\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/q06w72ls4kz7lg_wsqqw2f0h0000gp/T/ipykernel_96591/3840881219.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n"
     ]
    }
   ],
   "source": [
    "from metrics import character_accuracy_np as char_acc \n",
    "\n",
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "def compute_metrics(pred_ids, label_ids):\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    # tmp_hat_test = [np.asarray(list(x)) for x in pred_str]\n",
    "    # pred_str = np.asarray(tmp_hat_test)\n",
    "    # tmp_label_test = [np.asarray(list(x)) for x in label_str]\n",
    "    # label_str = np.asarray(tmp_label_test)\n",
    "    # acc = char_acc(label_str, pred_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def manual_eval(model, processor, images):\n",
    "    output = []\n",
    "    px_vals = []\n",
    "    for image in tqdm(images):\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "        px_vals.append(pixel_values)\n",
    "    for pixel_values in tqdm(px_vals):\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        output.append(generated_text)\n",
    "    return np.asarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/imgaug/imgaug.py:184: DeprecationWarning: Function `ContrastNormalization()` is deprecated. Use `imgaug.contrast.LinearContrast` instead.\n",
      "  warn_deprecated(msg, stacklevel=3)\n",
      "Augment Images:   0%|          | 0/637 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augment Images: 100%|██████████| 637/637 [00:51<00:00, 12.45it/s]\n",
      "Epoch: 1/5:  10%|█         | 113/1115 [05:52<56:11,  3.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 113: 1.032955126556675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5:  20%|██        | 225/1115 [11:41<48:16,  3.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 225: 0.8649246018462711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5:  30%|███       | 337/1115 [17:28<39:42,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 337: 0.7328139740962657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5:  40%|████      | 449/1115 [24:32<37:31,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 449: 0.6765433798040075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5:  50%|█████     | 561/1115 [31:04<30:06,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after batch 561: 0.5831750251034046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5:  52%|█████▏    | 585/1115 [32:25<32:09,  3.64s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=6.5e-6, no_deprecation_warning=True)\n",
    "max_epoch = 5\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.85)\n",
    "\n",
    "\n",
    "prev_loss = 2e5\n",
    "for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
    "    num_aug = 7-(epoch*2)\n",
    "    if num_aug < 1:\n",
    "        num_aug = 1\n",
    "    # Generate augmented image data for each epoch\n",
    "    if epoch >= 0:\n",
    "        augmenter = AugmentImage(image_array=train_img_array, label_array=train_label_array, num_augmentations=num_aug)\n",
    "        train_image_array, train_lbl_array = augmenter.transform()\n",
    "    else:\n",
    "        train_image_array = train_img_array\n",
    "        train_lbl_array = train_label_array\n",
    "    train_ds = BDC_Dataset(train_image_array, train_lbl_array, processor=processor)\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    early_stop = False\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader, desc=f'Epoch: {epoch+1}/{max_epoch}')):\n",
    "        # get the inputs\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if idx % (1 + (len(train_dataloader)//10)) == 0 and idx != 0:\n",
    "            prev_loss = train_loss/(idx+1)\n",
    "            print(f\"Loss after batch {(idx+1)}:\", train_loss/(idx+1))\n",
    "            for i in range(epoch+1):\n",
    "                scheduler.step()\n",
    "        if prev_loss*1.2 < train_loss/(idx+1):\n",
    "            early_stop = True\n",
    "            break\n",
    "    if early_stop:\n",
    "        print(f\"Loss after epoch {epoch}:\", train_loss/(idx+1))\n",
    "        prev_loss = train_loss/(idx+1)\n",
    "    else:\n",
    "        print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "        prev_loss = train_loss/(idx+1)\n",
    "    for i in range((max_epoch+epoch-2)*5):\n",
    "        scheduler.step()\n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    valid_cer = 0.0\n",
    "    valid_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        yhat_test = manual_eval(model=model, processor=processor, images = test_img_array)\n",
    "        tmp_hat_test = [np.asarray(list(x)) for x in yhat_test]\n",
    "        yhat_test = np.asarray(tmp_hat_test)\n",
    "        tmp_label_test = [np.asarray(list(x)) for x in test_label_array]\n",
    "        ylabel_test = np.asarray(tmp_label_test)\n",
    "        valid_acc = char_acc(ylabel_test, yhat_test)\n",
    "\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            # run batch generation\n",
    "            outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "            # compute metrics\n",
    "            cer = compute_metrics(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            valid_cer += cer \n",
    "        \n",
    "\n",
    "    print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
    "    print(\"Validation Char_acc:\", valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs\u001b[39m.\u001b[39;49mloss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9381944444444444"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'{MODEL_NAME}d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def manual_eval(model, processor, images):\n",
    "    output = []\n",
    "    px_vals = []\n",
    "    for image in tqdm(images):\n",
    "        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to('cpu')\n",
    "        px_vals.append(pixel_values)\n",
    "    for pixel_values in tqdm(px_vals):\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        output.append(generated_text)\n",
    "    return np.asarray(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdc_model = VisionEncoderDecoderModel.from_pretrained('trocr-base-printed_bdc_license_plates_ocrV2f').to('mps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_img_array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m character_accuracy_np\n\u001b[1;32m      3\u001b[0m \u001b[39m# yhat_train = manual_eval(model=bdc_model, processor=processor, images = train_image_array)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# tmp_hat = [np.asarray(list(x)) for x in yhat_train]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# yhat_train = np.asarray(tmp_hat)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# print(f\"Train dataset char_acc = {character_accuracy_np(train_label_array, yhat_train)}\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m yhat_test \u001b[39m=\u001b[39m manual_eval(model\u001b[39m=\u001b[39mbdc_model, processor\u001b[39m=\u001b[39mprocessor, images \u001b[39m=\u001b[39m test_img_array)\n\u001b[1;32m      9\u001b[0m tmp_hat_test \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39masarray(\u001b[39mlist\u001b[39m(x)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m yhat_test]\n\u001b[1;32m     10\u001b[0m yhat_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(tmp_hat_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_img_array' is not defined"
     ]
    }
   ],
   "source": [
    "from metrics import character_accuracy_np\n",
    "\n",
    "# yhat_train = manual_eval(model=bdc_model, processor=processor, images = train_image_array)\n",
    "# tmp_hat = [np.asarray(list(x)) for x in yhat_train]\n",
    "# yhat_train = np.asarray(tmp_hat)\n",
    "# print(f\"Train dataset char_acc = {character_accuracy_np(train_label_array, yhat_train)}\")\n",
    "\n",
    "yhat_test = manual_eval(model=model, processor=processor, images = test_img_array)\n",
    "tmp_hat_test = [np.asarray(list(x)) for x in yhat_test]\n",
    "yhat_test = np.asarray(tmp_hat_test)\n",
    "tmp_label_test = [np.asarray(list(x)) for x in test_label_array]\n",
    "ylabel_test = np.asarray(tmp_label_test)\n",
    "print(f\"test dataset char_acc = {character_accuracy_np(ylabel_test, yhat_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initial Preprocess: Add Padding: 100%|██████████| 100/100 [00:00<00:00, 689.15it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 70.33it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]/Users/mdaniyalk/miniforge3/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (12) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [02:37<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset compared to GroundTruth char_acc = 0.9655555555555554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/var/folders/vt/q06w72ls4kz7lg_wsqqw2f0h0000gp/T/ipykernel_96591/2541768004.py:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  yhat_test = np.asarray(tmp_hat_test)\n",
      "/var/folders/vt/q06w72ls4kz7lg_wsqqw2f0h0000gp/T/ipykernel_96591/2541768004.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ylabel_test = np.asarray(tmp_label_test)\n"
     ]
    }
   ],
   "source": [
    "df_gt = pd.read_csv('/Users/mdaniyalk/Downloads/marsel-ground_truth_manual.csv')\n",
    "gt = df_gt['Ground Truth'].to_numpy()\n",
    "gt_test_path_array = df_gt['Name of File'].to_numpy()\n",
    "\n",
    "gt_test_image = PreprocessImage(img_paths=gt_test_path_array, output_pixel=384, padding_ratio=0.99, path_prefix='test', include_original=False)\n",
    "gt_test_img_array = gt_test_image.img_array.copy()\n",
    "# gt_test_img_array = gt_test_image.gray_image(masked_img=np.array([]))\n",
    "yhat_test = manual_eval(model=bdc_model, processor=processor, images = gt_test_img_array)\n",
    "\n",
    "tmp_hat_test = [np.asarray(list(x)) for x in yhat_test]\n",
    "yhat_test = np.asarray(tmp_hat_test)\n",
    "tmp_label_test = [np.asarray(list(x)) for x in gt]\n",
    "ylabel_test = np.asarray(tmp_label_test)\n",
    "print(f\"test dataset compared to GroundTruth char_acc = {char_acc(ylabel_test, yhat_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset compared to GroundTruth char_acc = 0.9655555555555554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vt/q06w72ls4kz7lg_wsqqw2f0h0000gp/T/ipykernel_96591/3795801008.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ylabel_test = np.asarray(tmp_label_test)\n"
     ]
    }
   ],
   "source": [
    "df_gt = pd.read_csv('/Users/mdaniyalk/Downloads/marsel-ground_truth_manual.csv')\n",
    "gt = df_gt['Ground Truth'].to_numpy()\n",
    "\n",
    "tmp_label_test = [np.asarray(list(x)) for x in gt]\n",
    "ylabel_test = np.asarray(tmp_label_test)\n",
    "print(f\"test dataset compared to GroundTruth char_acc = {char_acc(ylabel_test, yhat_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AD7034BE', 'A9388EX', 'B16TB', 'B1661TKZ', 'AD3772ABE', 'B1373HV',\n",
       "       'B1064TFR', 'B1395TJW', 'B1270RFD', 'B1736BYH', 'B1627BIE',\n",
       "       'B1678WZM', 'AD9313SS', 'B1036UL', 'B1801TZS', 'B1474TJS',\n",
       "       'B1939PU', 'B1260TZT', 'B1376TJB', 'B17QO', 'AB6632ULZ',\n",
       "       'B1236PZM', 'AB8644PK', 'B1KT', 'AA7084QO', 'B1131EKG', 'B1163TJN',\n",
       "       'B1036UL', 'AB8820CB', 'A9192ZM', 'B0120KXT', 'B1643TRO',\n",
       "       'B1390TJQ', 'AB2681VD', 'B1860EFT', 'A8749FS', 'B1566FON',\n",
       "       'B1063SJQ', 'B1254TFX', 'B1038JY', 'AB4923QH', 'B1509UN',\n",
       "       'B1937TLP', 'B1202UL', 'B1026TMZ', 'B1724PYW', 'B1102SIV',\n",
       "       'AD7034OF', 'AB4352CX', 'B1895EJB', 'B1786UJT', 'B1549RFS',\n",
       "       'B1869EOF', 'B1713VY', 'B1063SPW', 'B1661TKZ', 'A8014VA',\n",
       "       'B1873YU', 'B1820UL', 'B1422BKI', 'AB5278XA', 'AD418U', 'B1157Y',\n",
       "       'B1233RFD', 'B1031NI', 'AD99JR', 'B1683SEY', 'AA7014OF',\n",
       "       'B1241SSW', 'B1632TJJ', 'B1907ELR', 'B1815TJQ', 'B1734UJN',\n",
       "       'B1743EYF', 'B1075QO', 'AB2933TM', 'B1523TJT', 'B1157SSL',\n",
       "       'B1713VY', 'B1361TJS', 'A9388EX', 'B1532NKY', 'B1800UYU',\n",
       "       'B1411TVE', 'B3787KE', 'B1339RFD', 'B1903RFD', 'AB2958QH',\n",
       "       'AD418U', 'B1885TLP', 'B1634SSN', 'AB3511OH', 'B1726UUA',\n",
       "       'B1820TJV', 'B1619DRB', 'B1265UL', 'AB8644PK', 'AG9718EG',\n",
       "       'B1509UN', 'B1408RX'], dtype='<U9')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "for gt_y in yhat_test:\n",
    "    tmp = []\n",
    "    for aa in gt_y:\n",
    "        if aa != '[PAD]':\n",
    "            tmp.append(aa)\n",
    "            \n",
    "    str_value = ''.join(tmp)\n",
    "    test.append(str_value)\n",
    "test = np.asarray(test)\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of File</th>\n",
       "      <th>Vehicleregistrationplatebymodel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DataTest1.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DataTest2.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DataTest3.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DataTest4.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DataTest5.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name of File  Vehicleregistrationplatebymodel\n",
       "0  DataTest1.png                              NaN\n",
       "1  DataTest2.png                              NaN\n",
       "2  DataTest3.png                              NaN\n",
       "3  DataTest4.png                              NaN\n",
       "4  DataTest5.png                              NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm = pd.read_csv('submission_sample.csv')\n",
    "subm = subm.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of File</th>\n",
       "      <th>Vehicleregistrationplatebymodel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DataTest1.png</td>\n",
       "      <td>AD7034OF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DataTest2.png</td>\n",
       "      <td>A9388EX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DataTest3.png</td>\n",
       "      <td>B16TB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DataTest4.png</td>\n",
       "      <td>B1661TKZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DataTest5.png</td>\n",
       "      <td>AD3772ABE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name of File Vehicleregistrationplatebymodel\n",
       "0  DataTest1.png                        AD7034OF\n",
       "1  DataTest2.png                         A9388EX\n",
       "2  DataTest3.png                           B16TB\n",
       "3  DataTest4.png                        B1661TKZ\n",
       "4  DataTest5.png                       AD3772ABE"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subm['Vehicleregistrationplatebymodel'] = test\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.to_csv('submission_modelV1.csv', index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes & Other Takeaways From This Project\n",
    "****\n",
    "- The results were pretty good. I was pondering whether to train for 2 or 3 epochs. Ultimately, I trained this model for 2 epochs. If this were a work project (where multiprocessing and other options are available), I would have trained for 3, if not 4, epochs.\n",
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "##### For Transformer Checkpoint\n",
    "- @misc{li2021trocr,\n",
    "      title={TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models}, \n",
    "      author={Minghao Li and Tengchao Lv and Lei Cui and Yijuan Lu and Dinei Florencio and Cha Zhang and Zhoujun Li and Furu Wei},\n",
    "      year={2021},\n",
    "      eprint={2109.10282},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CL}\n",
    "}\n",
    "\n",
    "##### For CER Metric\n",
    "- @inproceedings{morris2004,\n",
    "author = {Morris, Andrew and Maier, Viktoria and Green, Phil},\n",
    "year = {2004},\n",
    "month = {01},\n",
    "pages = {},\n",
    "title = {From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition.}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "41bc52750e0704433c7c40a5c68d8f60e760babe95f2dffc82e8c3790208ff57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
