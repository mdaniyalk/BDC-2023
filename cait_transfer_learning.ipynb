{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "89B27-TGiDNB"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLellw57Vlz2"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow-addons tensorflow-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u3d4Z7uQsmp"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "from utils import calculate_mean, calculate_std, pad_image"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mPo10cahZXXQ"
      },
      "source": [
        "## GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpvUOuC3j27n"
      },
      "outputs": [],
      "source": [
        "# try:  # detect TPUs\n",
        "#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # TPU detection\n",
        "#     strategy = tf.distribute.TPUStrategy(tpu)\n",
        "# except ValueError:  # detect GPUs\n",
        "#     tpu = False\n",
        "#     strategy = (\n",
        "#         tf.distribute.get_strategy()\n",
        "#     )  # default strategy that works on CPU and single GPU\n",
        "# print(\"Number of Accelerators: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w9S3uKC_iXY5"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Find the list of all fine-tunable models [here](https://tfhub.dev/sayakpaul/collections/cait/1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCc6tdUGnD4C"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "IMAGE_SIZE = [224, 224]\n",
        "MODEL_PATH = \"https://tfhub.dev/sayakpaul/cait_s24_224_fe/1\"\n",
        "\n",
        "# TPU\n",
        "# if tpu:\n",
        "#     BATCH_SIZE = (\n",
        "#         16 * strategy.num_replicas_in_sync\n",
        "#     )  # a TPU has 8 cores so this will be 128\n",
        "# else:\n",
        "BATCH_SIZE = 64  # on Colab/GPU, a higher batch size may throw(OOM)\n",
        "\n",
        "# Dataset\n",
        "CLASS_0 = ['A', 'AA', 'AB', 'AD', 'AE', 'AF', 'AG', 'B']\n",
        "CLASS_1 = [f'{i}' for i in range(10)]\n",
        "CLASS_2 = [f'{i}' for i in range(10)]\n",
        "CLASS_2.append('[PAD]')\n",
        "CLASS_3 = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[PAD]']\n",
        "\n",
        "\n",
        "# Other constants\n",
        "\n",
        "MEAN = tf.constant(calculate_mean('train'))  # bdc_train mean\n",
        "STD = tf.constant(calculate_std('train'))  # bdc_train std\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9iTImGI5qMQT"
      },
      "source": [
        "# Data Pipeline\n",
        "\n",
        "[CaiT authors](https://arxiv.org/abs/2103.17239) use a separate preprocessing pipeline for fine-tuning. But for keeping this walkthrough short and simple, we can just perform the basic ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h29TLx7gqN_7"
      },
      "outputs": [],
      "source": [
        "def make_dataset(dataset: tf.data.Dataset, train: bool, image_size: int = IMAGE_SIZE):\n",
        "    def preprocess(image, label):\n",
        "        # for training, do augmentation\n",
        "        if train:\n",
        "            if tf.random.uniform(shape=[]) > 0.5:\n",
        "                image = tf.image.flip_left_right(image)\n",
        "        image = tf.image.resize(image, size=image_size, method=\"bicubic\")\n",
        "        image = (image - MEAN) / STD  # normalization\n",
        "        return image, label\n",
        "\n",
        "    if train:\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 10)\n",
        "\n",
        "    return dataset.map(preprocess, AUTO).batch(BATCH_SIZE).prefetch(AUTO)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AMQ3Qs9_pddU"
      },
      "source": [
        "# BDC-2023 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('train/DataTrain.csv', delimiter=';')\n",
        "df_train = df_train.drop(['Unnamed: 0'], axis=1)\n",
        "df_train = df_train.drop([126, 457, 600]) # delete the wrong labeled data\n",
        "df_train.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path_train = df_train['NameofFile'].to_numpy()\n",
        "folder = 'train'\n",
        "img_list = [pad_image(f'{folder}/{pth}') for pth in path_train]\n",
        "img_list = np.asarray(img_list)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the label data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plate_train = df_train['Vehicleregistrationplate'].to_numpy()\n",
        "classes = [[], [], [], [], [], [], [], []]\n",
        "\n",
        "\n",
        "for string in plate_train:\n",
        "    match = re.match(r'([A-Za-z]*)(\\d*)([A-Za-z]*)', string)\n",
        "    if match:\n",
        "        alphabet_before_numeric = match.group(1)\n",
        "        numeric_value = match.group(2)\n",
        "        remaining_alphabet = match.group(3)\n",
        "        classes[0].append(CLASS_0.index(alphabet_before_numeric))\n",
        "        tmp_num = list(numeric_value)\n",
        "        if len(tmp_num) < 4:\n",
        "            for i in range(4-len(tmp_num)):\n",
        "                tmp_num.append(\"[PAD]\")\n",
        "        for i in range(1, 5):\n",
        "            if i == 1:\n",
        "                classes[i].append(CLASS_1.index(tmp_num[i-1]))\n",
        "            else:\n",
        "                classes[i].append(CLASS_2.index(tmp_num[i-1]))\n",
        "        tmp_rem = list(remaining_alphabet)\n",
        "        if len(tmp_rem) < 3:\n",
        "            for i in range(3-len(tmp_rem)):\n",
        "                tmp_rem.append(\"[PAD]\")\n",
        "        for i in range(5, 8):\n",
        "            classes[i].append(CLASS_3.index(tmp_rem[i-5]))\n",
        "        \n",
        "label_list = np.asarray(classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_list.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tmp data preparation\n",
        "mean = calculate_mean('train')*255\n",
        "std = calculate_std('train')*255\n",
        "img_list = (img_list - mean) / std\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.max(img_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONq1BjAEacAc"
      },
      "outputs": [],
      "source": [
        "# image_array = np.asarray(img_list)\n",
        "# train_image = image_array[:540]\n",
        "# val_image = image_array[540:]\n",
        "# # Convert the image array to TensorFlow dataset\n",
        "# train_image_dataset = tf.data.Dataset.from_tensor_slices(train_image)\n",
        "# val_image_dataset = tf.data.Dataset.from_tensor_slices(val_image)\n",
        "\n",
        "# # Convert the list of labels to NumPy array\n",
        "# label_array = np.asarray(label_list)\n",
        "# train_label = label_array[:,:540]\n",
        "# val_label = label_array[:,540:]\n",
        "# # Convert the label array to TensorFlow dataset\n",
        "# train_label_dataset = tf.data.Dataset.from_tensor_slices(train_label)\n",
        "# val_label_dataset = tf.data.Dataset.from_tensor_slices(val_label)\n",
        "\n",
        "# # Combine the image and label datasets into a single dataset\n",
        "# train_dataset = tf.data.Dataset.zip((train_image_dataset, train_label_dataset))\n",
        "# val_dataset = tf.data.Dataset.zip((val_image_dataset, val_label_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_label_dataset.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3G-2aUBQJ-H"
      },
      "outputs": [],
      "source": [
        "num_train = tf.data.experimental.cardinality(train_dataset)\n",
        "num_val = tf.data.experimental.cardinality(val_dataset)\n",
        "print(f\"Number of training examples: {num_train}\")\n",
        "print(f\"Number of validation examples: {num_val}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l2X7sE3oRLXN"
      },
      "source": [
        "## Prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oftrfYw1qXei"
      },
      "outputs": [],
      "source": [
        "train_dataset = make_dataset(train_dataset, True)\n",
        "val_dataset = make_dataset(val_dataset, False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kNyCCM6PRM8I"
      },
      "source": [
        "## Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaGzFUUVqjaC"
      },
      "outputs": [],
      "source": [
        "sample_images, sample_labels = next(iter(train_dataset))\n",
        "\n",
        "plt.figure(figsize=(5 * 3, 3 * 3))\n",
        "for n in range(15):\n",
        "    ax = plt.subplot(3, 5, n + 1)\n",
        "    image = (sample_images[n] * STD + MEAN).numpy()\n",
        "    image = (image - image.min()) / (\n",
        "        image.max() - image.min()\n",
        "    )  # convert to [0, 1] for avoiding matplotlib warning\n",
        "    plt.imshow(image)\n",
        "    plt.title(CLASSES[sample_labels[n]])\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ALtRUlxhw8Vt"
      },
      "source": [
        "# Model Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD9SI_Q9JdAB"
      },
      "outputs": [],
      "source": [
        "def number_output(x):\n",
        "    out_num_0 = tf.keras.layers.Dense(10, activation=\"softmax\", name='outputs_num_0')(x)\n",
        "    out_num_1 = tf.keras.layers.Dense(11, activation=\"softmax\", name='outputs_num_1')(x)\n",
        "    out_num_2 = tf.keras.layers.Dense(11, activation=\"softmax\", name='outputs_num_2')(x)\n",
        "    out_num_3 = tf.keras.layers.Dense(11, activation=\"softmax\", name='outputs_num_3')(x)\n",
        "    return out_num_0, out_num_1, out_num_2, out_num_3\n",
        "\n",
        "def char_output(x):\n",
        "    out_char_0 = tf.keras.layers.Dense(27, activation=\"softmax\", name='outputs_char_0')(x)\n",
        "    out_char_1 = tf.keras.layers.Dense(27, activation=\"softmax\", name='outputs_char_1')(x)\n",
        "    out_char_2 = tf.keras.layers.Dense(27, activation=\"softmax\", name='outputs_char_2')(x)\n",
        "    return out_char_0, out_char_1, out_char_2\n",
        "\n",
        "\n",
        "def get_model(\n",
        "    model_url: str, res: int = IMAGE_SIZE[0], num_classes: int = 5\n",
        ") -> tf.keras.Model:\n",
        "    inputs = tf.keras.Input((res, res, 3))\n",
        "    hub_module = hub.KerasLayer(model_url, trainable=False)\n",
        "\n",
        "    x, _, _ = hub_module(inputs)  # Second and third outputs in the tuple is a\n",
        "    # dictionary containing attention scores.\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    out = tf.keras.layers.Dense(8, activation=\"softmax\", name='outputs_code_area')(x)\n",
        "    out_num_0, out_num_1, out_num_2, out_num_3 = number_output(x)\n",
        "    out_char_0, out_char_1, out_char_2 = char_output(x)\n",
        "    outputs = [out, out_num_0, out_num_1, out_num_2,\n",
        "               out_num_3,  out_char_0, out_char_1, out_char_2]\n",
        "    return tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpZApp9u9_Y-"
      },
      "outputs": [],
      "source": [
        "model = get_model(MODEL_PATH)\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfenMQcxAAb"
      },
      "source": [
        "# Training Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D7Iu7oD8WzX"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "start_lr = 0.00001\n",
        "min_lr = 0.00001\n",
        "max_lr = 0.0002\n",
        "rampup_epochs = 5\n",
        "sustain_epochs = 0\n",
        "exp_decay = 0.8\n",
        "\n",
        "\n",
        "def lrfn(epoch):\n",
        "    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "        if epoch < rampup_epochs:\n",
        "            lr = (max_lr - start_lr) / rampup_epochs * epoch + start_lr\n",
        "        elif epoch < rampup_epochs + sustain_epochs:\n",
        "            lr = max_lr\n",
        "        else:\n",
        "            lr = (max_lr - min_lr) * exp_decay ** (\n",
        "                epoch - rampup_epochs - sustain_epochs\n",
        "            ) + min_lr\n",
        "        return lr\n",
        "\n",
        "    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(\n",
        "    lambda epoch: lrfn(epoch), verbose=True\n",
        ")\n",
        "\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [lrfn(x) for x in rng]\n",
        "plt.plot(rng, [lrfn(x) for x in rng])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roz3jse5ez09"
      },
      "outputs": [],
      "source": [
        "def bdc_loss(y_true, y_pred):\n",
        "    num_outputs = len(y_true)\n",
        "    losses = []\n",
        "    loss_names = []\n",
        "    for i in range(num_outputs):\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true[i], y_pred[i])\n",
        "        losses.append(loss)\n",
        "        loss_name = f\"loss_{i}\"\n",
        "        loss_names.append(loss_name)\n",
        "\n",
        "    # Average the losses\n",
        "    total_loss = tf.reduce_mean(losses, name=\"total_loss\")\n",
        "\n",
        "    # Add loss names as additional metrics\n",
        "    for i, loss_name in enumerate(loss_names):\n",
        "        tf.keras.metrics.Mean(name=loss_name)(losses[i])\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def character_accuracy_tf(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the character-based accuracy using TensorFlow.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (tensorflow.Tensor): Ground truth labels.\n",
        "        y_pred (tensorflow.Tensor): Predicted labels.\n",
        "\n",
        "    Returns:\n",
        "        tensorflow.Tensor: Character-based accuracy.\n",
        "\n",
        "    \"\"\"\n",
        "    y_true_flat = tf.reshape(y_true, [-1])\n",
        "    y_pred_flat = tf.reshape(y_pred, [-1])\n",
        "\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(y_true_flat, y_pred_flat), tf.float32))\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ki3b7YhghTc8"
      },
      "outputs": [],
      "source": [
        "metrics = {'outputs_code_area':tf.keras.metrics.CategoricalAccuracy(),\n",
        "           'outputs_num_0':tf.keras.metrics.CategoricalAccuracy(),\n",
        "           'outputs_num_1':tf.keras.metrics.CategoricalAccuracy(),\n",
        "           'outputs_num_2':tf.keras.metrics.CategoricalAccuracy(),\n",
        "           'outputs_num_3':tf.keras.metrics.CategoricalAccuracy(),\n",
        "           'outputs_char_0':tf.keras.metrics.CategoricalAccuracy(),\n",
        "           'outputs_char_1':tf.keras.metrics.CategoricalAccuracy(),\n",
        "           'outputs_char_2':tf.keras.metrics.CategoricalAccuracy(),\n",
        "           }\n",
        "optimizer = tfa.optimizers.AdamW(weight_decay=1e-5)\n",
        "loss = [tf.keras.losses.SparseCategoricalCrossentropy() for i in range(8)]\n",
        "loss_weight = [0.125 for i in range(8)]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E9p4ymNh9y7d"
      },
      "source": [
        "# Training & Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnZTSd8K90Mq"
      },
      "outputs": [],
      "source": [
        "# with strategy.scope():  # this line is all that is needed to run on TPU (or multi-GPU, ...)\n",
        "# model = get_model(MODEL_PATH)\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=metrics, loss_weights=loss_weight)\n",
        "\n",
        "# history = model.fit(\n",
        "#     train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=[lr_callback]\n",
        "# )\n",
        "label = [l for l in label_list]\n",
        "history = model.fit(\n",
        "    img_list, label, validation_split=0.2, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[lr_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc7LMVz5Cbx6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "result = pd.DataFrame(history.history)\n",
        "fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n",
        "result[[\"accuracy\", \"val_accuracy\"]].plot(xlabel=\"epoch\", ylabel=\"score\", ax=ax[0])\n",
        "result[[\"loss\", \"val_loss\"]].plot(xlabel=\"epoch\", ylabel=\"score\", ax=ax[1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MKFMWzh0Yxsq"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMEsR851VDZb"
      },
      "outputs": [],
      "source": [
        "sample_images, sample_labels = next(iter(val_dataset))\n",
        "\n",
        "predictions = model.predict(sample_images, batch_size=16).argmax(axis=-1)\n",
        "evaluations = model.evaluate(sample_images, sample_labels, batch_size=16)\n",
        "\n",
        "print(\"[val_loss, val_acc]\", evaluations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzCCDL1CZFx6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5 * 3, 3 * 3))\n",
        "for n in range(15):\n",
        "    ax = plt.subplot(3, 5, n + 1)\n",
        "    image = (sample_images[n] * STD + MEAN).numpy()\n",
        "    image = (image - image.min()) / (\n",
        "        image.max() - image.min()\n",
        "    )  # convert to [0, 1] for avoiding matplotlib warning\n",
        "    plt.imshow(image)\n",
        "    target = CLASSES[sample_labels[n]]\n",
        "    pred = CLASSES[predictions[n]]\n",
        "    plt.title(\"{} ({})\".format(target, pred))\n",
        "    plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-8.m91",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
